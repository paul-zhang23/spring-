"""
ç®€å•å®éªŒè„šæœ¬ï¼šå¯¹æ¯”æœ‰å›¾è°± vs æ— å›¾è°±çš„RAGæ•ˆæœ
è¿è¡Œæ–¹å¼: python run_experiment.py
"""

import json
import time
from pathlib import Path
from collections import Counter

# å¯¼å…¥ç³»ç»Ÿæ¨¡å—
from config import (
    DATA_FILE, EMBEDDING_MODEL_NAME, GENERATION_MODEL_NAME, TOP_K,
    MILVUS_LITE_DATA_PATH, COLLECTION_NAME, id_to_doc_map
)
from data_utils import load_data
from models import load_embedding_model, load_generation_model
from milvus_utils import get_milvus_client, setup_milvus_collection, index_data_if_needed, search_similar_documents
from rag_core import generate_answer

# åŠ è½½çŸ¥è¯†å›¾è°±æ„å»ºå‡½æ•°
import networkx as nx
import re

# ============== ç®€åŒ–ç‰ˆçŸ¥è¯†å›¾è°±æ„å»ºï¼ˆä»app.pyå¤åˆ¶ï¼‰ ==============
STOPWORDS = {
    'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
    'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'been',
    'be', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',
    'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that',
    'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they'
}

MEDICAL_WHITELIST = {
    'bcc', 'uv', 'ct', 'mri', 'pet', 'dna', 'rna', 'hiv', 'aids', 'copd',
    'skin', 'face', 'head', 'neck', 'eye', 'eyes', 'age', 'body', 'cell',
    'basal cell carcinoma', 'squamous cell carcinoma', 'melanoma',
    'carcinoma', 'cancer', 'lymphoma', 'tumor', 'disease', 'syndrome',
    'surgery', 'radiation therapy', 'chemotherapy', 'systemic therapy',
    'treatment', 'biopsy', 'uv radiation', 'sun exposure', 'fair skin',
    'immune suppression', 'tanning beds', 'lymph nodes', 'brain',
    'basal cells', 'epidermis'
}

def is_valid_entity(entity, seen_entities):
    entity_lower = entity.lower().strip()
    if entity_lower in seen_entities:
        return False
    if entity_lower in MEDICAL_WHITELIST:
        return True
    if entity_lower in STOPWORDS:
        return False
    if len(entity_lower) < 3:
        return False
    if not any(c.isalpha() for c in entity_lower):
        return False
    return True

def build_knowledge_graph(corpus_path="GraphRAG-Benchmark-main/Datasets/Corpus/medical.json", max_sentences=80):
    """æ„å»ºçŸ¥è¯†å›¾è°±"""
    if not Path(corpus_path).exists():
        print(f"âš ï¸  è¯­æ–™åº“æ–‡ä»¶ä¸å­˜åœ¨: {corpus_path}")
        return None

    with open(corpus_path, 'r', encoding='utf-8') as f:
        corpus_data = json.load(f)

    context = corpus_data.get('context', '')
    graph = nx.DiGraph()
    seen_entities = set()
    entity_types = {}

    # ç–¾ç—…å®ä½“è¯†åˆ«
    disease_pattern = r'\b(?:[A-Z][a-z]+\s+)?(?:basal\s+cell\s+|squamous\s+cell\s+)?(?:carcinoma|cancer|lymphoma|tumor|disease|syndrome)\b'
    for match in re.finditer(disease_pattern, context, re.IGNORECASE):
        disease = match.group(0).strip()
        if is_valid_entity(disease, seen_entities):
            graph.add_node(disease, type='Disease', color='#e74c3c')
            entity_types[disease] = 'Disease'
            seen_entities.add(disease.lower())

    # è§£å‰–ä½ç½®è¯†åˆ«
    anatomy_keywords = ['skin', 'face', 'head', 'neck', 'lymph nodes', 'brain',
                       'eyes', 'basal cells', 'epidermis', 'body']
    for anatomy in anatomy_keywords:
        if anatomy.lower() in context.lower() and is_valid_entity(anatomy, seen_entities):
            graph.add_node(anatomy.title(), type='Anatomy', color='#3498db')
            entity_types[anatomy.title()] = 'Anatomy'
            seen_entities.add(anatomy.lower())

    # æ²»ç–—æ–¹æ³•è¯†åˆ«
    treatment_keywords = ['surgery', 'radiation therapy', 'chemotherapy',
                         'systemic therapy', 'treatment', 'biopsy']
    for treatment in treatment_keywords:
        if treatment.lower() in context.lower() and is_valid_entity(treatment, seen_entities):
            graph.add_node(treatment.title(), type='Treatment', color='#2ecc71')
            entity_types[treatment.title()] = 'Treatment'
            seen_entities.add(treatment.lower())

    # é£é™©å› ç´ è¯†åˆ«
    risk_keywords = ['UV radiation', 'sun exposure', 'fair skin', 'age',
                    'immune suppression', 'tanning beds']
    for risk in risk_keywords:
        if risk.lower() in context.lower() and is_valid_entity(risk, seen_entities):
            graph.add_node(risk.title(), type='RiskFactor', color='#e67e22')
            entity_types[risk.title()] = 'RiskFactor'
            seen_entities.add(risk.lower())

    # åŸºäºå…±ç°æ·»åŠ è¾¹
    sentences = context.split('.')[:max_sentences]
    nodes_list = list(graph.nodes())

    for sentence in sentences:
        entities_in_sentence = []
        for node in nodes_list:
            if node.lower() in sentence.lower():
                entities_in_sentence.append(node)

        if len(entities_in_sentence) >= 2:
            for i in range(len(entities_in_sentence) - 1):
                for j in range(i + 1, len(entities_in_sentence)):
                    source = entities_in_sentence[i]
                    target = entities_in_sentence[j]

                    source_type = entity_types.get(source, 'Other')
                    target_type = entity_types.get(target, 'Other')

                    if source_type == 'RiskFactor' and target_type == 'Disease':
                        relation = 'risk_factor_for'
                    elif source_type == 'Treatment' and target_type == 'Disease':
                        relation = 'treats'
                    elif source_type == 'Disease' and target_type == 'Anatomy':
                        relation = 'affects'
                    else:
                        relation = 'related_to'

                    graph.add_edge(source, target, relation=relation)

    return graph

def extract_graph_context(query, graph):
    """ä»çŸ¥è¯†å›¾è°±æå–ç›¸å…³ä¸Šä¸‹æ–‡"""
    if not graph:
        return ""

    # è¯†åˆ«æŸ¥è¯¢ä¸­çš„å®ä½“
    entities_found = []
    for node in graph.nodes():
        if node.lower() in query.lower():
            entities_found.append(node)

    if not entities_found:
        return ""

    # è·å–å®ä½“é‚»å±…
    graph_info = []
    for entity in entities_found[:3]:  # æœ€å¤š3ä¸ªå®ä½“
        neighbors = list(graph.successors(entity)) + list(graph.predecessors(entity))
        if neighbors:
            graph_info.append(f"Entity '{entity}' is related to: {', '.join(neighbors[:5])}")

    return "\n".join(graph_info) if graph_info else ""

# ============== ç®€å•è¯„ä¼°æŒ‡æ ‡ ==============
def calculate_word_overlap(text1, text2):
    """è®¡ç®—è¯é‡å ç‡"""
    words1 = set(text1.lower().split())
    words2 = set(text2.lower().split())
    if not words1 or not words2:
        return 0.0
    overlap = len(words1 & words2)
    return overlap / max(len(words1), len(words2))

def simple_rouge_1(generated, reference):
    """ç®€åŒ–çš„ROUGE-1åˆ†æ•°"""
    gen_words = generated.lower().split()
    ref_words = reference.lower().split()

    if not gen_words or not ref_words:
        return 0.0

    # è®¡ç®—å¬å›ç‡
    matches = sum(1 for word in ref_words if word in gen_words)
    recall = matches / len(ref_words)

    # è®¡ç®—ç²¾ç¡®ç‡
    precision = matches / len(gen_words) if len(gen_words) > 0 else 0

    # F1åˆ†æ•°
    if precision + recall == 0:
        return 0.0
    f1 = 2 * (precision * recall) / (precision + recall)

    return f1

# ============== ä¸»å®éªŒå‡½æ•° ==============
def run_experiment(num_questions=10):
    """
    è¿è¡Œå¯¹æ¯”å®éªŒ

    Args:
        num_questions: æµ‹è¯•é—®é¢˜æ•°é‡ï¼ˆé»˜è®¤10é¢˜ï¼‰
    """
    print("=" * 60)
    print("ğŸ§ª GraphRAG å¯¹æ¯”å®éªŒ")
    print("=" * 60)

    # 1. åŠ è½½æµ‹è¯•é—®é¢˜
    questions_file = "GraphRAG-Benchmark-main/Datasets/Questions/medical_questions.json"
    if not Path(questions_file).exists():
        print(f"âŒ æµ‹è¯•é—®é¢˜æ–‡ä»¶ä¸å­˜åœ¨: {questions_file}")
        return

    with open(questions_file, 'r', encoding='utf-8') as f:
        questions_data = json.load(f)

    print(f"ğŸ“‹ åŠ è½½ {len(questions_data)} ä¸ªæµ‹è¯•é—®é¢˜ï¼Œå°†æµ‹è¯•å‰ {num_questions} é¢˜")

    # 2. åˆå§‹åŒ–æ¨¡å‹å’Œæ•°æ®åº“
    print("\nğŸ”§ åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶...")

    print("  - åŠ è½½åµŒå…¥æ¨¡å‹...")
    embedding_model = load_embedding_model()

    print("  - åŠ è½½ç”Ÿæˆæ¨¡å‹...")
    generation_model, tokenizer = load_generation_model()

    print("  - è¿æ¥å‘é‡æ•°æ®åº“...")
    milvus_client = get_milvus_client()
    if not milvus_client:
        print("âŒ Milvuså®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥")
        return

    print("  - è®¾ç½®æ•°æ®é›†åˆ...")
    setup_milvus_collection(milvus_client)

    print("  - åŠ è½½æ–‡æ¡£æ•°æ®...")
    data = load_data()

    print("  - ç´¢å¼•æ–‡æ¡£ï¼ˆå¦‚éœ€è¦ï¼‰...")
    index_data_if_needed(milvus_client, data, embedding_model)

    # 3. æ„å»ºçŸ¥è¯†å›¾è°±
    print("\nğŸŒ æ„å»ºçŸ¥è¯†å›¾è°±...")
    knowledge_graph = build_knowledge_graph()

    if knowledge_graph:
        num_nodes = knowledge_graph.number_of_nodes()
        num_edges = knowledge_graph.number_of_edges()
        print(f"  âœ… å›¾è°±æ„å»ºå®Œæˆ: {num_nodes} ä¸ªèŠ‚ç‚¹, {num_edges} æ¡è¾¹")
    else:
        print("  âš ï¸  å›¾è°±æ„å»ºå¤±è´¥ï¼Œå°†ä»…æµ‹è¯•ä¼ ç»ŸRAG")

    # 4. è¿è¡Œå®éªŒ
    print("\n" + "=" * 60)
    print("ğŸš€ å¼€å§‹å®éªŒ...")
    print("=" * 60)

    results = {
        'traditional_rag': [],  # æ— å›¾è°±
        'graph_rag': []         # æœ‰å›¾è°±
    }

    for i, qa in enumerate(questions_data[:num_questions]):
        print(f"\nğŸ“ é—®é¢˜ {i+1}/{num_questions}: {qa['question'][:60]}...")

        # æ£€ç´¢æ–‡æ¡£
        start_time = time.time()
        retrieved_ids, distances = search_similar_documents(
            milvus_client, qa['question'], embedding_model
        )
        retrieval_time = time.time() - start_time

        if not retrieved_ids:
            print("  âš ï¸  æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£ï¼Œè·³è¿‡")
            continue

        retrieved_docs = [id_to_doc_map[doc_id] for doc_id in retrieved_ids if doc_id in id_to_doc_map]

        if not retrieved_docs:
            print("  âš ï¸  æ–‡æ¡£æ˜ å°„å¤±è´¥ï¼Œè·³è¿‡")
            continue

        # æ–¹æ¡ˆ1: ä¼ ç»ŸRAGï¼ˆæ— å›¾è°±ï¼‰
        print("  ğŸ”¹ ä¼ ç»ŸRAGç”Ÿæˆä¸­...")
        start_gen = time.time()
        traditional_answer = generate_answer(
            qa['question'], retrieved_docs, generation_model, tokenizer
        )
        trad_gen_time = time.time() - start_gen

        # æ–¹æ¡ˆ2: GraphRAGï¼ˆæœ‰å›¾è°±ï¼‰
        graph_answer = None
        graph_gen_time = 0

        if knowledge_graph:
            print("  ğŸ”¹ GraphRAGç”Ÿæˆä¸­...")

            # æå–å›¾è°±ä¸Šä¸‹æ–‡
            graph_context = extract_graph_context(qa['question'], knowledge_graph)

            # å¢å¼ºæ–‡æ¡£ï¼ˆæ·»åŠ å›¾è°±ä¿¡æ¯ï¼‰
            enhanced_docs = retrieved_docs.copy()
            if graph_context:
                enhanced_docs.append({
                    'title': 'Knowledge Graph Context',
                    'content': f"Graph Information:\n{graph_context}"
                })

            start_gen = time.time()
            graph_answer = generate_answer(
                qa['question'], enhanced_docs, generation_model, tokenizer
            )
            graph_gen_time = time.time() - start_gen

        # è·å–æ ‡å‡†ç­”æ¡ˆ
        ground_truth = qa.get('evidence', [''])[0] if isinstance(qa.get('evidence'), list) else qa.get('evidence', '')

        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        trad_overlap = calculate_word_overlap(traditional_answer, ground_truth)
        trad_rouge = simple_rouge_1(traditional_answer, ground_truth)

        results['traditional_rag'].append({
            'question': qa['question'],
            'answer': traditional_answer,
            'ground_truth': ground_truth,
            'word_overlap': trad_overlap,
            'rouge_1': trad_rouge,
            'retrieval_time': retrieval_time,
            'generation_time': trad_gen_time
        })

        print(f"    ä¼ ç»ŸRAG - è¯é‡å : {trad_overlap:.2%}, ROUGE-1: {trad_rouge:.3f}")

        if graph_answer:
            graph_overlap = calculate_word_overlap(graph_answer, ground_truth)
            graph_rouge = simple_rouge_1(graph_answer, ground_truth)

            results['graph_rag'].append({
                'question': qa['question'],
                'answer': graph_answer,
                'ground_truth': ground_truth,
                'word_overlap': graph_overlap,
                'rouge_1': graph_rouge,
                'retrieval_time': retrieval_time,
                'generation_time': graph_gen_time
            })

            print(f"    GraphRAG  - è¯é‡å : {graph_overlap:.2%}, ROUGE-1: {graph_rouge:.3f}")

            # å¯¹æ¯”
            if graph_rouge > trad_rouge:
                print(f"    âœ… GraphRAG æ›´ä¼˜ (+{(graph_rouge - trad_rouge):.3f})")
            elif graph_rouge < trad_rouge:
                print(f"    âš ï¸  ä¼ ç»ŸRAG æ›´ä¼˜ (+{(trad_rouge - graph_rouge):.3f})")
            else:
                print(f"    â– ä¸¤è€…ç›¸åŒ")

    # 5. æ±‡æ€»ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š å®éªŒç»“æœæ±‡æ€»")
    print("=" * 60)

    if results['traditional_rag']:
        trad_avg_overlap = sum(r['word_overlap'] for r in results['traditional_rag']) / len(results['traditional_rag'])
        trad_avg_rouge = sum(r['rouge_1'] for r in results['traditional_rag']) / len(results['traditional_rag'])
        trad_avg_time = sum(r['generation_time'] for r in results['traditional_rag']) / len(results['traditional_rag'])

        print(f"\nğŸ”¹ ä¼ ç»ŸRAG (æ— å›¾è°±):")
        print(f"   å¹³å‡è¯é‡å ç‡: {trad_avg_overlap:.2%}")
        print(f"   å¹³å‡ROUGE-1:  {trad_avg_rouge:.3f}")
        print(f"   å¹³å‡ç”Ÿæˆæ—¶é—´: {trad_avg_time:.2f}s")

    if results['graph_rag']:
        graph_avg_overlap = sum(r['word_overlap'] for r in results['graph_rag']) / len(results['graph_rag'])
        graph_avg_rouge = sum(r['rouge_1'] for r in results['graph_rag']) / len(results['graph_rag'])
        graph_avg_time = sum(r['generation_time'] for r in results['graph_rag']) / len(results['graph_rag'])

        print(f"\nğŸ”¹ GraphRAG (æœ‰å›¾è°±):")
        print(f"   å¹³å‡è¯é‡å ç‡: {graph_avg_overlap:.2%}")
        print(f"   å¹³å‡ROUGE-1:  {graph_avg_rouge:.3f}")
        print(f"   å¹³å‡ç”Ÿæˆæ—¶é—´: {graph_avg_time:.2f}s")

        # å¯¹æ¯”æå‡
        overlap_improvement = ((graph_avg_overlap - trad_avg_overlap) / trad_avg_overlap * 100) if trad_avg_overlap > 0 else 0
        rouge_improvement = ((graph_avg_rouge - trad_avg_rouge) / trad_avg_rouge * 100) if trad_avg_rouge > 0 else 0

        print(f"\nğŸ“ˆ GraphRAG ç›¸å¯¹æå‡:")
        print(f"   è¯é‡å ç‡: {overlap_improvement:+.1f}%")
        print(f"   ROUGE-1:  {rouge_improvement:+.1f}%")
        print(f"   ç”Ÿæˆæ—¶é—´: {((graph_avg_time - trad_avg_time) / trad_avg_time * 100):+.1f}%")

    # 6. ä¿å­˜ç»“æœ
    output_file = "experiment_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            'traditional_rag': results['traditional_rag'],
            'graph_rag': results['graph_rag'],
            'summary': {
                'traditional_rag': {
                    'avg_word_overlap': trad_avg_overlap if results['traditional_rag'] else 0,
                    'avg_rouge_1': trad_avg_rouge if results['traditional_rag'] else 0,
                    'avg_gen_time': trad_avg_time if results['traditional_rag'] else 0
                },
                'graph_rag': {
                    'avg_word_overlap': graph_avg_overlap if results['graph_rag'] else 0,
                    'avg_rouge_1': graph_avg_rouge if results['graph_rag'] else 0,
                    'avg_gen_time': graph_avg_time if results['graph_rag'] else 0
                } if results['graph_rag'] else None
            }
        }, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ’¾ å®éªŒç»“æœå·²ä¿å­˜è‡³: {output_file}")
    print("\nâœ… å®éªŒå®Œæˆï¼")

if __name__ == "__main__":
    # è¿è¡Œå®éªŒï¼Œæµ‹è¯•å‰10é¢˜
    run_experiment(num_questions=10)
