import streamlit as st
import time
import os
from pathlib import Path

# ============== ã€ä¿®å¤ã€‘ä½¿ç”¨ç»å¯¹è·¯å¾„è®¾ç½®ç¯å¢ƒå˜é‡ ==============
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HF_HOME'] = str(Path(__file__).parent.absolute() / 'hf_cache')
# ====================================================

# ============== ã€æ–°å¢ã€‘çŸ¥è¯†å›¾è°±ç›¸å…³å¯¼å…¥ ==============
import json
import re
import networkx as nx
from pyvis.network import Network
import streamlit.components.v1 as components
from difflib import SequenceMatcher  # ç”¨äºæ¨¡ç³ŠåŒ¹é…
# ====================================================

# ============== ã€æ–°å¢ã€‘æ¨¡å‹å¯¹æ¯”ç›¸å…³å¯¼å…¥ ==============
from typing import List, Dict, Tuple
# ====================================================

# Import functions and config from other modules
from config import (
    DATA_FILE, EMBEDDING_MODEL_NAME, GENERATION_MODEL_NAME, TOP_K,
    MAX_ARTICLES_TO_INDEX, MILVUS_LITE_DATA_PATH, COLLECTION_NAME,
    id_to_doc_map # Import the global map
)
from data_utils import load_data
from models import load_embedding_model, load_generation_model
# Import the new Milvus Lite functions
from milvus_utils import get_milvus_client, setup_milvus_collection, index_data_if_needed, search_similar_documents
from rag_core import generate_answer

# ============== ã€æ–°å¢ã€‘çŸ¥è¯†å›¾è°±é™å™ªå·¥å…·å‡½æ•° ==============
# è‹±æ–‡åœç”¨è¯åˆ—è¡¨
STOPWORDS = {
    'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
    'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'been',
    'be', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',
    'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that',
    'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they'
}

# åŒ»å­¦å…³é”®è¯ç™½åå•ï¼ˆä¿ç•™è¿™äº›å³ä½¿å¾ˆçŸ­ï¼‰
MEDICAL_WHITELIST = {
    'bcc', 'uv', 'ct', 'mri', 'pet', 'dna', 'rna', 'hiv', 'aids', 'copd',
    'skin', 'face', 'head', 'neck', 'eye', 'eyes', 'age', 'body', 'cell',
    'basal cell carcinoma', 'squamous cell carcinoma', 'melanoma',
    'carcinoma', 'cancer', 'lymphoma', 'tumor', 'disease', 'syndrome',
    'surgery', 'radiation therapy', 'chemotherapy', 'systemic therapy',
    'treatment', 'biopsy', 'uv radiation', 'sun exposure', 'fair skin',
    'immune suppression', 'tanning beds', 'lymph nodes', 'brain',
    'basal cells', 'epidermis'
}

def is_valid_entity(entity, seen_entities):
    """
    éªŒè¯å®ä½“æ˜¯å¦æœ‰æ•ˆï¼ˆé™å™ªï¼‰

    Args:
        entity: å€™é€‰å®ä½“
        seen_entities: å·²æ·»åŠ çš„å®ä½“é›†åˆï¼ˆç”¨äºå»é‡ï¼‰

    Returns:
        bool: æ˜¯å¦æœ‰æ•ˆ
    """
    entity_lower = entity.lower().strip()

    # å»é™¤é‡å¤
    if entity_lower in seen_entities:
        return False

    # ç™½åå•ç›´æ¥é€šè¿‡
    if entity_lower in MEDICAL_WHITELIST:
        return True

    # è¿‡æ»¤åœç”¨è¯
    if entity_lower in STOPWORDS:
        return False

    # è¿‡æ»¤è¿‡çŸ­è¯ï¼ˆä½†ä¿ç•™ç™½åå•ï¼‰
    if len(entity_lower) < 3:
        return False

    # è¿‡æ»¤çº¯æ•°å­—æˆ–ç‰¹æ®Šå­—ç¬¦
    if not any(c.isalpha() for c in entity_lower):
        return False

    return True
# ====================================================

# ============== ã€æ–°å¢+ä¿®æ”¹ã€‘çŸ¥è¯†å›¾è°±æ„å»ºå‡½æ•°ï¼ˆåŠ å…¥é™å™ªï¼‰ ==============
@st.cache_resource
def build_knowledge_graph(
    corpus_path="GraphRAG-Benchmark-main/Datasets/Corpus/medical.json",
    max_sentences=80
):
    """
    ä»è¯­æ–™åº“ä¸­æ„å»ºçŸ¥è¯†å›¾è°±ï¼ˆå«é™å™ªï¼‰

    Args:
        corpus_path: è¯­æ–™åº“JSONæ–‡ä»¶è·¯å¾„
        max_sentences: ç”¨äºå…±ç°å…³ç³»çš„æœ€å¤§å¥å­æ•°ï¼ˆé¿å…å™ªå£°ï¼‰

    Returns:
        NetworkX DiGraphå¯¹è±¡
    """
    if not Path(corpus_path).exists():
        st.warning(f"è¯­æ–™åº“æ–‡ä»¶ä¸å­˜åœ¨: {corpus_path}")
        return None

    # åŠ è½½è¯­æ–™åº“
    with open(corpus_path, 'r', encoding='utf-8') as f:
        corpus_data = json.load(f)

    context = corpus_data.get('context', '')

    # åˆ›å»ºæœ‰å‘å›¾
    graph = nx.DiGraph()

    # å·²æ·»åŠ å®ä½“é›†åˆï¼ˆç”¨äºå»é‡ï¼‰
    seen_entities = set()
    entity_types = {}

    # ============== ã€ä¿®æ”¹ã€‘ç–¾ç—…å®ä½“è¯†åˆ«ï¼ˆåŠ å…¥é™å™ªï¼‰ ==============
    disease_pattern = (
        r'\b(?:[A-Z][a-z]+\s+)?'
        r'(?:basal\s+cell\s+|squamous\s+cell\s+)?'
        r'(?:carcinoma|cancer|lymphoma|tumor|disease|syndrome)\b'
    )
    for match in re.finditer(disease_pattern, context, re.IGNORECASE):
        disease = match.group(0).strip()
        if is_valid_entity(disease, seen_entities):
            graph.add_node(disease, type='Disease', color='#e74c3c')
            entity_types[disease] = 'Disease'
            seen_entities.add(disease.lower())
    # ====================================================

    # ============== ã€ä¿®æ”¹ã€‘è§£å‰–ä½ç½®è¯†åˆ«ï¼ˆåŠ å…¥é™å™ªï¼‰ ==============
    anatomy_keywords = ['skin', 'face', 'head', 'neck', 'lymph nodes', 'brain',
                       'eyes', 'basal cells', 'epidermis', 'body']

    for anatomy in anatomy_keywords:
        if anatomy.lower() in context.lower() and is_valid_entity(anatomy, seen_entities):
            graph.add_node(anatomy.title(), type='Anatomy', color='#3498db')
            entity_types[anatomy.title()] = 'Anatomy'
            seen_entities.add(anatomy.lower())
    # ====================================================

    # ============== ã€ä¿®æ”¹ã€‘æ²»ç–—æ–¹æ³•è¯†åˆ«ï¼ˆåŠ å…¥é™å™ªï¼‰ ==============
    treatment_keywords = ['surgery', 'radiation therapy', 'chemotherapy',
                         'systemic therapy', 'treatment', 'biopsy']

    for treatment in treatment_keywords:
        if treatment.lower() in context.lower() and is_valid_entity(treatment, seen_entities):
            graph.add_node(treatment.title(), type='Treatment', color='#2ecc71')
            entity_types[treatment.title()] = 'Treatment'
            seen_entities.add(treatment.lower())
    # ====================================================

    # ============== ã€ä¿®æ”¹ã€‘é£é™©å› ç´ è¯†åˆ«ï¼ˆåŠ å…¥é™å™ªï¼‰ ==============
    risk_keywords = ['UV radiation', 'sun exposure', 'fair skin', 'age',
                    'immune suppression', 'tanning beds']

    for risk in risk_keywords:
        if risk.lower() in context.lower() and is_valid_entity(risk, seen_entities):
            graph.add_node(risk.title(), type='RiskFactor', color='#e67e22')
            entity_types[risk.title()] = 'RiskFactor'
            seen_entities.add(risk.lower())
    # ====================================================

    # åŸºäºå…±ç°æ·»åŠ è¾¹ï¼ˆç®€åŒ–ç‰ˆå…³ç³»è¯†åˆ«ï¼‰
    sentences = context.split('.')[:max_sentences]  # åªå¤„ç†å‰Nå¥

    nodes_list = list(graph.nodes())
    for sentence in sentences:
        # æŸ¥æ‰¾å¥å­ä¸­å‡ºç°çš„å®ä½“
        entities_in_sentence = []
        for node in nodes_list:
            if node.lower() in sentence.lower():
                entities_in_sentence.append(node)

        # ä¸ºå…±ç°çš„å®ä½“æ·»åŠ è¾¹
        if len(entities_in_sentence) >= 2:
            for i in range(len(entities_in_sentence) - 1):
                for j in range(i + 1, len(entities_in_sentence)):
                    source = entities_in_sentence[i]
                    target = entities_in_sentence[j]

                    # æ ¹æ®ç±»å‹æ¨æ–­å…³ç³»
                    source_type = entity_types.get(source, 'Other')
                    target_type = entity_types.get(target, 'Other')

                    if source_type == 'RiskFactor' and target_type == 'Disease':
                        relation = 'risk_factor_for'
                    elif source_type == 'Treatment' and target_type == 'Disease':
                        relation = 'treats'
                    elif source_type == 'Disease' and target_type == 'Anatomy':
                        relation = 'affects'
                    else:
                        relation = 'related_to'

                    graph.add_edge(source, target, relation=relation)

    return graph
# ====================================================


def visualize_knowledge_subgraph(graph, center_entity, max_hops=1):
    """
    å¯è§†åŒ–å®ä½“å‘¨å›´çš„å­å›¾

    Args:
        graph: NetworkXå›¾å¯¹è±¡
        center_entity: ä¸­å¿ƒå®ä½“
        max_hops: æœ€å¤§è·³æ•°

    Returns:
        HTMLå­—ç¬¦ä¸²
    """
    if center_entity not in graph:
        return "<p>å®ä½“ä¸å­˜åœ¨äºçŸ¥è¯†å›¾è°±ä¸­</p>"

    # ä½¿ç”¨BFSè·å–æŒ‡å®šè·³æ•°å†…çš„èŠ‚ç‚¹
    subgraph_nodes = {center_entity}
    frontier = {center_entity}
    for _ in range(max_hops):
        next_frontier = set()
        for node in frontier:
            neighbors = set(graph.successors(node)) | set(graph.predecessors(node))
            next_frontier.update(neighbors)
        subgraph_nodes.update(next_frontier)
        frontier = next_frontier

    # é™åˆ¶èŠ‚ç‚¹æ•°é‡ï¼Œé¿å…å›¾è¿‡å¤§
    if len(subgraph_nodes) > 50:
        subgraph_nodes = set(list(subgraph_nodes)[:50])

    # åˆ›å»ºå­å›¾
    subgraph = graph.subgraph(subgraph_nodes).copy()

    # åˆ›å»ºPyVisç½‘ç»œ
    net = Network(height="500px", width="100%", directed=True,
                 bgcolor="#ffffff", font_color="#000000")

    # æ·»åŠ èŠ‚ç‚¹
    for node in subgraph.nodes():
        node_data = subgraph.nodes[node]
        color = node_data.get('color', '#95a5a6')

        # ä¸­å¿ƒèŠ‚ç‚¹ç‰¹æ®Šæ ‡è®°
        if node == center_entity:
            size = 30
            color = '#c0392b'
        else:
            size = 20

        net.add_node(node, label=node, title=node, color=color, size=size)

    # æ·»åŠ è¾¹
    for u, v, data in subgraph.edges(data=True):
        relation = data.get('relation', 'related_to')
        net.add_edge(u, v, label=relation, title=relation)

    # ç”ŸæˆHTML
    net.set_options("""
    {
      "physics": {
        "enabled": true,
        "stabilization": {"iterations": 100}
      }
    }
    """)

    html = net.generate_html()
    return html


def get_entity_info(graph, entity):
    """
    è·å–å®ä½“è¯¦ç»†ä¿¡æ¯

    Args:
        graph: NetworkXå›¾å¯¹è±¡
        entity: å®ä½“åç§°

    Returns:
        dict: å®ä½“ä¿¡æ¯
    """
    if entity not in graph:
        return None

    # è·å–é‚»å±…
    out_neighbors = list(graph.successors(entity))
    in_neighbors = list(graph.predecessors(entity))

    return {
        'entity': entity,
        'type': graph.nodes[entity].get('type', 'Unknown'),
        'out_degree': len(out_neighbors),
        'in_degree': len(in_neighbors),
        'out_neighbors': out_neighbors[:10],
        'in_neighbors': in_neighbors[:10]
    }

# ============== ã€æ–°å¢ã€‘æ¨¡ç³ŠåŒ¹é…æŸ¥æ‰¾å®ä½“ ==============
def fuzzy_search_entities(graph, keyword, threshold=0.6, max_results=10):
    """
    æ¨¡ç³ŠåŒ¹é…æŸ¥æ‰¾å®ä½“

    Args:
        graph: NetworkXå›¾å¯¹è±¡
        keyword: æœç´¢å…³é”®è¯
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰
        max_results: æœ€å¤§è¿”å›ç»“æœæ•°

    Returns:
        list: [(entity, similarity_score), ...]
    """
    if not keyword or not graph:
        return []

    keyword_lower = keyword.lower().strip()
    matches = []

    for node in graph.nodes():
        node_lower = node.lower()

        # é•¿åº¦çº¦æŸï¼Œé¿å…åŒ¹é…è¿‡é•¿æˆ–è¿‡çŸ­çš„å™ªå£°å®ä½“
        if len(node_lower) > max(60, len(keyword_lower) * 4):
            continue

        # ç²¾ç¡®åŒ¹é…
        if keyword_lower == node_lower:
            matches.append((node, 1.0))
        # å­ä¸²åŒ¹é…
        elif keyword_lower in node_lower or node_lower in keyword_lower:
            matches.append((node, 0.9))
        # æ¨¡ç³ŠåŒ¹é…
        else:
            similarity = SequenceMatcher(None, keyword_lower, node_lower).ratio()
            if similarity >= threshold:
                matches.append((node, similarity))

    # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
    matches.sort(key=lambda x: x[1], reverse=True)
    return matches[:max_results]
# ====================================================

# --- Streamlit UI è®¾ç½® ---
# ============== ã€ä¿®æ”¹ã€‘é¡µé¢é…ç½®ï¼Œæ·»åŠ çŸ¥è¯†å›¾è°±å›¾æ ‡ ==============
st.set_page_config(layout="wide", page_title="åŒ»ç–—RAG+çŸ¥è¯†å›¾è°±ç³»ç»Ÿ", page_icon="ğŸ¥")
# ====================================================

# ============== ã€ä¿®æ”¹ã€‘æ ‡é¢˜ï¼Œä½“ç°çŸ¥è¯†å›¾è°±åŠŸèƒ½ ==============
st.title("ğŸ¥ åŒ»ç–— RAG + çŸ¥è¯†å›¾è°±ç³»ç»Ÿ")
st.markdown(f"ä½¿ç”¨ Milvus Lite, `{EMBEDDING_MODEL_NAME}`, `{GENERATION_MODEL_NAME}` + **çŸ¥è¯†å›¾è°±å¢å¼º**")
# ====================================================

# ============== ã€æ–°å¢ã€‘åŠ è½½çŸ¥è¯†å›¾è°± ==============
st.sidebar.markdown("---")
st.sidebar.header("ğŸ•¸ï¸ çŸ¥è¯†å›¾è°±")
enable_kg = st.sidebar.checkbox("å¯ç”¨çŸ¥è¯†å›¾è°±", value=True, help="å¯ç”¨çŸ¥è¯†å›¾è°±åŠŸèƒ½è¿›è¡Œå¢å¼ºæ£€ç´¢")

# ============== ã€æ–°å¢ã€‘é—®ç­”è”åŠ¨å¼€å…³ ==============
enable_qa_linkage = st.sidebar.checkbox("å¯ç”¨é—®ç­”è”åŠ¨", value=True, help="é—®ç­”æ—¶è‡ªåŠ¨æ˜¾ç¤ºç›¸å…³å®ä½“çš„å­å›¾")
# ====================================================

knowledge_graph = None
if enable_kg:
    corpus_path = st.sidebar.selectbox(
        "é€‰æ‹©è¯­æ–™åº“",
        ["GraphRAG-Benchmark-main/Datasets/Corpus/medical.json",
         "GraphRAG-Benchmark-main/Datasets/Corpus/novel.json"]
    )
    max_sentences = st.sidebar.slider(
        "å›¾è°±å…±ç°å¥æ•°ä¸Šé™",
        min_value=20,
        max_value=200,
        value=80,
        step=20,
        help="é™åˆ¶ç”¨äºæ„å»ºå…±ç°å…³ç³»çš„å¥å­æ•°é‡ï¼Œé™ä½å™ªå£°"
    )

    with st.spinner("æ­£åœ¨æ„å»ºçŸ¥è¯†å›¾è°±..."):
        knowledge_graph = build_knowledge_graph(corpus_path, max_sentences)

    if knowledge_graph:
        st.sidebar.success(f"âœ… å›¾è°±åŠ è½½æˆåŠŸ")
        st.sidebar.metric("èŠ‚ç‚¹æ•°", knowledge_graph.number_of_nodes())
        st.sidebar.metric("è¾¹æ•°", knowledge_graph.number_of_edges())
    else:
        st.sidebar.error("âŒ å›¾è°±åŠ è½½å¤±è´¥")
# ====================================================

# --- åˆå§‹åŒ–ä¸ç¼“å­˜ ---
# è·å– Milvus Lite å®¢æˆ·ç«¯ (å¦‚æœæœªç¼“å­˜åˆ™åˆå§‹åŒ–)
milvus_client = get_milvus_client()

if milvus_client:
    # è®¾ç½® collection (å¦‚æœæœªç¼“å­˜åˆ™åˆ›å»º/åŠ è½½ç´¢å¼•)
    collection_is_ready = setup_milvus_collection(milvus_client)

    # åŠ è½½æ¨¡å‹ (ç¼“å­˜)
    embedding_model = load_embedding_model(EMBEDDING_MODEL_NAME)
    generation_model, tokenizer = load_generation_model(GENERATION_MODEL_NAME)

    # æ£€æŸ¥æ‰€æœ‰ç»„ä»¶æ˜¯å¦æˆåŠŸåŠ è½½
    models_loaded = embedding_model and generation_model and tokenizer

    if collection_is_ready and models_loaded:
        # ============== ã€ä¿®å¤ã€‘æ”¹è¿›æ•°æ®åŠ è½½é€»è¾‘ï¼Œå…è®¸ä½¿ç”¨å·²æœ‰ç´¢å¼• ==============
        # åŠ è½½æ•°æ® (æœªç¼“å­˜)
        pubmed_data = load_data(DATA_FILE)

        # å¦‚æœéœ€è¦åˆ™ç´¢å¼•æ•°æ® (è¿™ä¼šå¡«å…… id_to_doc_map)
        if pubmed_data:
            indexing_successful = index_data_if_needed(milvus_client, pubmed_data, embedding_model)
        else:
            st.warning(f"âš ï¸ æ— æ³•ä» {DATA_FILE} åŠ è½½æ•°æ®ã€‚")
            # æ£€æŸ¥æ˜¯å¦æœ‰å·²å­˜åœ¨çš„æ–‡æ¡£æ˜ å°„
            if id_to_doc_map:
                st.info("âœ… ä½¿ç”¨å·²æœ‰çš„æ–‡æ¡£æ˜ å°„ç»§ç»­è¿è¡Œï¼ˆä»ä¹‹å‰çš„ä¼šè¯åŠ è½½ï¼‰")
                indexing_successful = True
            else:
                st.error("âŒ æ²¡æœ‰å¯ç”¨çš„æ–‡æ¡£æ•°æ®ã€‚RAGåŠŸèƒ½å°†è¢«ç¦ç”¨ã€‚")
                indexing_successful = False
        # ====================================================

        st.divider()

        # ============== ã€æ–°å¢ã€‘åˆ›å»ºæ ‡ç­¾é¡µï¼ˆå¢åŠ æ¨¡å‹å¯¹æ¯”ï¼‰ ==============
        tab1, tab2, tab3 = st.tabs(["ğŸ’¬ æ™ºèƒ½é—®ç­”", "ğŸ•¸ï¸ çŸ¥è¯†å›¾è°±", "âš–ï¸ æ¨¡å‹å¯¹æ¯”"])
        # ====================================================

        # ============== ã€ä¿®æ”¹ã€‘å°†åŸæœ‰é—®ç­”åŠŸèƒ½æ”¾å…¥tab1 ==============
        with tab1:
            # --- RAG äº¤äº’éƒ¨åˆ† ---
            if not indexing_successful and not id_to_doc_map:
                 st.error("æ•°æ®ç´¢å¼•å¤±è´¥æˆ–ä¸å®Œæ•´ï¼Œä¸”æ²¡æœ‰æ–‡æ¡£æ˜ å°„ã€‚RAG åŠŸèƒ½å·²ç¦ç”¨ã€‚")
            else:
                # ============== ã€æ–°å¢ã€‘å¤šè½®å¯¹è¯åŠŸèƒ½ ==============
                # åˆå§‹åŒ–å¯¹è¯å†å²
                if 'conversation_history' not in st.session_state:
                    st.session_state.conversation_history = []
                if 'iteration_count' not in st.session_state:
                    st.session_state.iteration_count = 0

                # å¤šè½®å¯¹è¯å¼€å…³
                enable_multi_turn = st.sidebar.checkbox("å¯ç”¨å¤šè½®å¯¹è¯", value=False, help="å¯ç”¨åå¯ä»¥è¿›è¡Œä¸Šä¸‹æ–‡ç›¸å…³çš„å¤šè½®é—®ç­”")

                # æ˜¾ç¤ºå¯¹è¯å†å²
                if enable_multi_turn and st.session_state.conversation_history:
                    with st.expander("ğŸ“œ å¯¹è¯å†å²", expanded=False):
                        for i, turn in enumerate(st.session_state.conversation_history):
                            st.markdown(f"**ç¬¬ {i+1} è½®:**")
                            st.markdown(f"ğŸ‘¤ **ç”¨æˆ·**: {turn['question']}")
                            st.markdown(f"ğŸ¤– **AI**: {turn['answer'][:200]}{'...' if len(turn['answer']) > 200 else ''}")
                            st.markdown("---")

                        col1, col2 = st.columns(2)
                        with col1:
                            if st.button("ğŸ”„ é‡æ–°å¼€å§‹å¯¹è¯", key="reset_conversation"):
                                st.session_state.conversation_history = []
                                st.session_state.iteration_count = 0
                                st.rerun()
                        with col2:
                            st.metric("å¯¹è¯è½®æ•°", len(st.session_state.conversation_history))
                # ====================================================
                # ============== ã€æ–°å¢ã€‘ç¤ºä¾‹é—®é¢˜ ==============
                st.markdown("#### ğŸ’¡ ç¤ºä¾‹é—®é¢˜")
                example_cols = st.columns(3)
                examples = [
                    "What is basal cell carcinoma?",
                    "What are the risk factors?",
                    "How is it diagnosed?"
                ]
                for idx, (col, ex) in enumerate(zip(example_cols, examples)):
                    with col:
                        if st.button(ex, key=f"ex_{idx}"):
                            st.session_state.query_input = ex
                # ====================================================

                query = st.text_input(
                    "è¯·æå‡ºå…³äºå·²ç´¢å¼•åŒ»ç–—æ–‡ç« çš„é—®é¢˜:",
                    key="query_input",
                    value=st.session_state.get('query_input', '')
                )

                if st.button("ğŸš€ è·å–ç­”æ¡ˆ", key="submit_button", type="primary") and query:
                    start_time = time.time()

                    # 1. æœç´¢ Milvus Lite
                    with st.spinner("æ­£åœ¨æœç´¢ç›¸å…³æ–‡æ¡£..."):
                        retrieved_ids, distances = search_similar_documents(milvus_client, query, embedding_model)

                    if not retrieved_ids:
                        st.warning("åœ¨æ•°æ®åº“ä¸­æ‰¾ä¸åˆ°ç›¸å…³æ–‡æ¡£ã€‚")
                    else:
                        # ============== ã€ä¿®å¤ã€‘ä¿®å¤ç´¢å¼•æ˜ å°„é”™ä½é—®é¢˜ ==============
                        # 2. ä»æ˜ å°„ä¸­æ£€ç´¢ä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶ä¿æŒIDå’Œè·ç¦»çš„å¯¹åº”å…³ç³»
                        retrieved_docs = []
                        valid_ids = []
                        valid_distances = []

                        for i, doc_id in enumerate(retrieved_ids):
                            if doc_id in id_to_doc_map:
                                retrieved_docs.append(id_to_doc_map[doc_id])
                                valid_ids.append(doc_id)
                                if distances and i < len(distances):
                                    valid_distances.append(distances[i])
                        # ====================================================

                        if not retrieved_docs:
                             st.error("æ£€ç´¢åˆ°çš„ ID æ— æ³•æ˜ å°„åˆ°åŠ è½½çš„æ–‡æ¡£ã€‚è¯·æ£€æŸ¥æ˜ å°„é€»è¾‘ã€‚")
                        else:
                            # ============== ã€æ–°å¢ã€‘çŸ¥è¯†å›¾è°±å¢å¼ºï¼šæå–æŸ¥è¯¢ä¸­çš„å®ä½“ ==============
                            kg_entities = []
                            if enable_kg and knowledge_graph:
                                st.markdown("---")
                                st.markdown("#### ğŸ•¸ï¸ çŸ¥è¯†å›¾è°±å¢å¼ºä¿¡æ¯")

                                # ä»æŸ¥è¯¢ä¸­æå–å¯èƒ½çš„å®ä½“
                                query_lower = query.lower()
                                for node in knowledge_graph.nodes():
                                    if node.lower() in query_lower:
                                        kg_entities.append(node)

                                if kg_entities:
                                    st.info(f"è¯†åˆ«åˆ°ç›¸å…³å®ä½“: {', '.join(kg_entities[:3])}")

                                    # æ˜¾ç¤ºå®ä½“ä¿¡æ¯
                                    for entity in kg_entities[:2]:  # æœ€å¤šæ˜¾ç¤º2ä¸ª
                                        entity_info = get_entity_info(knowledge_graph, entity)
                                        if entity_info:
                                            with st.expander(f"ğŸ“ å®ä½“: {entity}", expanded=False):
                                                col1, col2, col3 = st.columns(3)
                                                col1.metric("ç±»å‹", entity_info['type'])
                                                col2.metric("å‡ºè¾¹", entity_info['out_degree'])
                                                col3.metric("å…¥è¾¹", entity_info['in_degree'])

                                                if entity_info['out_neighbors']:
                                                    st.markdown("**ç›¸å…³å®ä½“:** " + ", ".join(entity_info['out_neighbors'][:5]))
                                else:
                                    st.info("æœªåœ¨çŸ¥è¯†å›¾è°±ä¸­æ‰¾åˆ°ç›¸å…³å®ä½“")
                            # ====================================================

                            st.markdown("---")
                            st.subheader("ğŸ“š æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ–‡æ¡£")
                            for i, doc in enumerate(retrieved_docs):
                                # ============== ã€ä¿®æ”¹+ä¿®å¤ã€‘ä¼˜åŒ–æ–‡æ¡£å±•ç¤ºï¼Œä¿®å¤ç´¢å¼•æ˜ å°„ï¼Œæ”¯æŒä¸åŒåº¦é‡ç±»å‹ ==============
                                # ä½¿ç”¨valid_idså’Œvalid_distancesç¡®ä¿å¯¹åº”å…³ç³»æ­£ç¡®
                                if valid_distances and i < len(valid_distances):
                                    from config import INDEX_METRIC_TYPE
                                    dist_value = valid_distances[i]

                                    # æ ¹æ®åº¦é‡ç±»å‹æ˜¾ç¤ºä¸åŒçš„æ ‡ç­¾å’Œè®¡ç®—ç›¸ä¼¼åº¦
                                    if INDEX_METRIC_TYPE == "L2":
                                        metric_label = "è·ç¦»"
                                        similarity_pct = max(0, 100 * (1 - dist_value / 2))
                                    elif INDEX_METRIC_TYPE == "IP":
                                        metric_label = "å†…ç§¯"
                                        similarity_pct = max(0, min(100, dist_value * 100))  # IPè¶Šå¤§è¶Šç›¸ä¼¼
                                    else:  # COSINEç­‰
                                        metric_label = "ç›¸ä¼¼åº¦"
                                        similarity_pct = max(0, min(100, dist_value * 100))

                                    header = f"ğŸ“„ æ–‡æ¡£ {i+1} (ç›¸ä¼¼åº¦: {similarity_pct:.1f}%, {metric_label}: {dist_value:.4f}, ID: {valid_ids[i]}) - {doc['title'][:60]}"
                                else:
                                    header = f"ğŸ“„ æ–‡æ¡£ {i+1} (ID: {valid_ids[i] if i < len(valid_ids) else 'N/A'}) - {doc['title'][:60]}"

                                with st.expander(header, expanded=(i==0)):
                                    st.write(f"**æ ‡é¢˜:** {doc['title']}")
                                    st.write(f"**æ‘˜è¦:** {doc['abstract'][:500]}...")  # é™åˆ¶é•¿åº¦
                                # ====================================================

                            st.divider()

                            # 3. ç”Ÿæˆç­”æ¡ˆ
                            st.subheader("ğŸ¤– AIç”Ÿæˆç­”æ¡ˆ")
                            with st.spinner("æ­£åœ¨æ ¹æ®ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ..."):
                                answer = generate_answer(query, retrieved_docs, generation_model, tokenizer)
                                # ============== ã€ä¿®æ”¹ã€‘ä¼˜åŒ–ç­”æ¡ˆå±•ç¤º ==============
                                st.markdown(
                                    f"""
                                    <div style="background-color:#f0f9ff;padding:1.5rem;border-radius:0.5rem;border-left:4px solid #0284c7;">
                                        <p style="color:#0c4a6e;margin:0;">{answer}</p>
                                    </div>
                                    """,
                                    unsafe_allow_html=True
                                )
                                # ====================================================

                            # ============== ã€æ–°å¢ã€‘ä¿å­˜å¯¹è¯åˆ°å†å² ==============
                            if enable_multi_turn:
                                st.session_state.conversation_history.append({
                                    'question': query,
                                    'answer': answer,
                                    'retrieved_docs': len(retrieved_docs),
                                    'entities': kg_entities if kg_entities else []
                                })
                                st.session_state.iteration_count += 1
                            # ====================================================

                            # ============== ã€æ–°å¢ã€‘é—®ç­”è”åŠ¨ï¼šè‡ªåŠ¨æ˜¾ç¤ºå®ä½“å­å›¾ ==============
                            if enable_kg and knowledge_graph and enable_qa_linkage and kg_entities:
                                st.markdown("---")
                                st.markdown("#### ğŸ¯ é—®ç­”è”åŠ¨ï¼šç›¸å…³å®ä½“å­å›¾")
                                st.info(f"æ­£åœ¨å±•ç¤ºä¸é—®é¢˜ç›¸å…³çš„å®ä½“å­å›¾ (å¯åœ¨ä¾§è¾¹æ å…³é—­'å¯ç”¨é—®ç­”è”åŠ¨')")

                                # é€‰æ‹©æœ€ç›¸å…³çš„å®ä½“ (ç¬¬ä¸€ä¸ªè¯†åˆ«åˆ°çš„)
                                primary_entity = kg_entities[0]

                                with st.expander(f"ğŸ”— {primary_entity} çš„å…³ç³»ç½‘ç»œ", expanded=True):
                                    st.markdown(f"**ä¸­å¿ƒå®ä½“:** `{primary_entity}`")

                                    # ç”Ÿæˆå¹¶å±•ç¤ºå­å›¾
                                    with st.spinner(f"æ­£åœ¨åŠ è½½ {primary_entity} çš„å…³ç³»å›¾è°±..."):
                                        html = visualize_knowledge_subgraph(knowledge_graph, primary_entity, max_hops=1)
                                        components.html(html, height=520, scrolling=True)

                                    # æ˜¾ç¤ºè¯¥å®ä½“çš„é‚»å±…ä¿¡æ¯
                                    entity_info = get_entity_info(knowledge_graph, primary_entity)
                                    if entity_info and (entity_info['out_neighbors'] or entity_info['in_neighbors']):
                                        st.markdown("---")
                                        col1, col2 = st.columns(2)
                                        with col1:
                                            if entity_info['out_neighbors']:
                                                st.markdown("**å…³è”å®ä½“:**")
                                                for neighbor in entity_info['out_neighbors'][:5]:
                                                    st.markdown(f"- {neighbor}")
                                        with col2:
                                            if entity_info['in_neighbors']:
                                                st.markdown("**è¢«å…³è”è€…:**")
                                                for neighbor in entity_info['in_neighbors'][:5]:
                                                    st.markdown(f"- {neighbor}")
                            # ====================================================

                    end_time = time.time()

                    # ============== ã€ä¿®æ”¹ã€‘æ·»åŠ æ›´å¤šæ€§èƒ½æŒ‡æ ‡ ==============
                    st.markdown("---")
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("â±ï¸ æ€»è€—æ—¶", f"{end_time - start_time:.2f}s")
                    with col2:
                        st.metric("ğŸ“„ æ£€ç´¢æ–‡æ¡£æ•°", len(retrieved_docs) if retrieved_ids else 0)
                    with col3:
                        st.metric("ğŸ•¸ï¸ å›¾è°±å®ä½“æ•°", len(kg_entities) if kg_entities else 0)
                    # ====================================================

        # ============== ã€æ–°å¢ã€‘çŸ¥è¯†å›¾è°±å¯è§†åŒ–æ ‡ç­¾é¡µ ==============
        with tab2:
            if not enable_kg or not knowledge_graph:
                st.warning("âš ï¸ çŸ¥è¯†å›¾è°±æœªå¯ç”¨æˆ–æœªåŠ è½½")
                st.info("è¯·åœ¨ä¾§è¾¹æ å‹¾é€‰'å¯ç”¨çŸ¥è¯†å›¾è°±'")
            else:
                st.markdown("### ğŸ•¸ï¸ çŸ¥è¯†å›¾è°±å¯è§†åŒ–")

                # å›¾è°±ç»Ÿè®¡
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("èŠ‚ç‚¹æ€»æ•°", knowledge_graph.number_of_nodes())
                with col2:
                    st.metric("è¾¹æ€»æ•°", knowledge_graph.number_of_edges())
                with col3:
                    avg_degree = sum(dict(knowledge_graph.degree()).values()) / max(knowledge_graph.number_of_nodes(), 1)
                    st.metric("å¹³å‡åº¦æ•°", f"{avg_degree:.2f}")

                st.markdown("---")

                # ============== ã€æ–°å¢+ä¿®æ”¹ã€‘ä¸»åŠ¨æŸ¥è¯¢å®ä½“ï¼ˆæ”¯æŒæ¨¡ç³ŠåŒ¹é…ï¼‰ ==============
                st.markdown("#### ğŸ” ä¸»åŠ¨æŸ¥è¯¢å®ä½“")

                # è¾“å…¥å…³é”®è¯
                search_keyword = st.text_input(
                    "è¾“å…¥å…³é”®è¯æœç´¢å®ä½“ (æ”¯æŒæ¨¡ç³ŠåŒ¹é…)",
                    key="entity_search_keyword",
                    placeholder="ä¾‹å¦‚: cancer, skin, treatment..."
                )

                if search_keyword:
                    with st.spinner("æ­£åœ¨æœç´¢å®ä½“..."):
                        search_results = fuzzy_search_entities(knowledge_graph, search_keyword, threshold=0.6, max_results=10)

                    if search_results:
                        st.success(f"æ‰¾åˆ° {len(search_results)} ä¸ªåŒ¹é…å®ä½“")

                        # æ˜¾ç¤ºåŒ¹é…å€™é€‰
                        st.markdown("**åŒ¹é…å®ä½“åˆ—è¡¨:**")
                        candidate_entities = []
                        for entity, score in search_results:
                            candidate_entities.append(f"{entity} (ç›¸ä¼¼åº¦: {score:.2f})")

                        selected_candidate = st.selectbox(
                            "é€‰æ‹©è¦æ¢ç´¢çš„å®ä½“",
                            options=search_results,
                            format_func=lambda x: f"{x[0]} (ç›¸ä¼¼åº¦: {x[1]:.2f})",
                            key="fuzzy_search_selectbox"
                        )

                        if selected_candidate:
                            selected_entity = selected_candidate[0]

                            # æ˜¾ç¤ºå®ä½“ä¿¡æ¯
                            entity_info = get_entity_info(knowledge_graph, selected_entity)

                            if entity_info:
                                st.markdown(f"### ğŸ“Œ {entity_info['entity']}")
                                st.markdown(f"**ç±»å‹**: `{entity_info['type']}`")

                                col1, col2 = st.columns(2)
                                with col1:
                                    st.metric("å‡ºè¾¹æ•°é‡", entity_info['out_degree'])
                                    if entity_info['out_neighbors']:
                                        st.markdown("**ç›¸å…³å®ä½“:**")
                                        for neighbor in entity_info['out_neighbors'][:5]:
                                            st.markdown(f"- {neighbor}")

                                with col2:
                                    st.metric("å…¥è¾¹æ•°é‡", entity_info['in_degree'])
                                    if entity_info['in_neighbors']:
                                        st.markdown("**è¢«å…³è”å®ä½“:**")
                                        for neighbor in entity_info['in_neighbors'][:5]:
                                            st.markdown(f"- {neighbor}")

                            # å¯è§†åŒ–å­å›¾
                            st.markdown("---")
                            st.markdown("#### ğŸ¨ å®ä½“å­å›¾å¯è§†åŒ–")

                            if st.button("ğŸ”® ç”Ÿæˆå¯è§†åŒ–", key="fuzzy_search_visualize", type="primary"):
                                with st.spinner("æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–..."):
                                    html = visualize_knowledge_subgraph(knowledge_graph, selected_entity, max_hops=1)
                                    components.html(html, height=520, scrolling=True)
                    else:
                        st.warning(f"æœªæ‰¾åˆ°ä¸ '{search_keyword}' åŒ¹é…çš„å®ä½“ï¼Œè¯·å°è¯•å…¶ä»–å…³é”®è¯")
                else:
                    # æ²¡æœ‰è¾“å…¥å…³é”®è¯æ—¶ï¼Œæ˜¾ç¤ºåŸæœ‰çš„ä¸‹æ‹‰é€‰æ‹©
                    st.markdown("**æˆ–ä»æ‰€æœ‰å®ä½“ä¸­é€‰æ‹©:**")
                    all_nodes = sorted(list(knowledge_graph.nodes()))

                    if all_nodes:
                        selected_entity = st.selectbox(
                            "é€‰æ‹©è¦æ¢ç´¢çš„å®ä½“",
                            options=all_nodes,
                            index=0,
                            key="all_entities_selectbox"
                        )

                        if selected_entity:
                            # æ˜¾ç¤ºå®ä½“ä¿¡æ¯
                            entity_info = get_entity_info(knowledge_graph, selected_entity)

                            if entity_info:
                                st.markdown(f"### ğŸ“Œ {entity_info['entity']}")
                                st.markdown(f"**ç±»å‹**: `{entity_info['type']}`")

                                col1, col2 = st.columns(2)
                                with col1:
                                    st.metric("å‡ºè¾¹æ•°é‡", entity_info['out_degree'])
                                    if entity_info['out_neighbors']:
                                        st.markdown("**ç›¸å…³å®ä½“:**")
                                        for neighbor in entity_info['out_neighbors'][:5]:
                                            st.markdown(f"- {neighbor}")

                                with col2:
                                    st.metric("å…¥è¾¹æ•°é‡", entity_info['in_degree'])
                                    if entity_info['in_neighbors']:
                                        st.markdown("**è¢«å…³è”å®ä½“:**")
                                        for neighbor in entity_info['in_neighbors'][:5]:
                                            st.markdown(f"- {neighbor}")

                            # å¯è§†åŒ–
                            st.markdown("---")
                            st.markdown("#### ğŸ¨ å­å›¾å¯è§†åŒ–")

                            if st.button("ğŸ”® ç”Ÿæˆå¯è§†åŒ–", key="all_entities_visualize", type="primary"):
                                with st.spinner("æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–..."):
                                    html = visualize_knowledge_subgraph(knowledge_graph, selected_entity, max_hops=1)
                                    components.html(html, height=520, scrolling=True)
                    else:
                        st.info("çŸ¥è¯†å›¾è°±ä¸­æš‚æ— èŠ‚ç‚¹")
                # ====================================================
        # ====================================================

        # ============== ã€æ–°å¢ã€‘æ¨¡å‹å¯¹æ¯”æ ‡ç­¾é¡µ ==============
        with tab3:
            st.markdown("### âš–ï¸ DeepSeek ä¸æœ¬åœ°æ¨¡å‹å‡†ç¡®åº¦å¯¹æ¯”")
            st.markdown(
                "å¯¹æ¯”åŒä¸€æ‰¹é—®é¢˜çš„ä¸¤ç§ç­”æ¡ˆã€‚è¯·å…ˆç”Ÿæˆå¯¹åº”çš„ç»“æœæ–‡ä»¶ä¸è¯„æµ‹ç»“æœæ–‡ä»¶ã€‚"
            )

            col1, col2 = st.columns(2)
            with col1:
                local_result_path = st.text_input(
                    "æœ¬åœ°æ¨¡å‹ç»“æœæ–‡ä»¶",
                    value="GraphRAG-Benchmark-main/results/easy_rag_medical_50.json",
                    help="åŒ…å«ç”Ÿæˆç­”æ¡ˆçš„JSONæ–‡ä»¶"
                )
                local_eval_path = st.text_input(
                    "æœ¬åœ°æ¨¡å‹è¯„æµ‹ç»“æœ",
                    value="GraphRAG-Benchmark-main/results/eval_generation.json",
                    help="generation_eval çš„è¾“å‡ºç»“æœ"
                )
            with col2:
                deepseek_result_path = st.text_input(
                    "DeepSeekç»“æœæ–‡ä»¶",
                    value="GraphRAG-Benchmark-main/results/deepseek_medical_50.json",
                    help="åŒ…å«ç”Ÿæˆç­”æ¡ˆçš„JSONæ–‡ä»¶"
                )
                deepseek_eval_path = st.text_input(
                    "DeepSeekè¯„æµ‹ç»“æœ",
                    value="GraphRAG-Benchmark-main/results/eval_deepseek.json",
                    help="generation_eval çš„è¾“å‡ºç»“æœ"
                )

            @st.cache_data(show_spinner=False)
            def load_json_file(path):
                try:
                    with open(path, "r", encoding="utf-8") as f:
                        return json.load(f)
                except Exception:
                    return None

            def extract_scores(eval_data):
                if not eval_data:
                    return None
                scores = eval_data.get("Fact Retrieval")
                if not isinstance(scores, dict):
                    return None
                return {
                    "rouge_score": scores.get("rouge_score"),
                    "answer_correctness": scores.get("answer_correctness"),
                }

            def format_score(value):
                if value is None or (isinstance(value, float) and value != value):
                    return "N/A"
                return f"{value:.4f}"

            local_eval = load_json_file(local_eval_path)
            deepseek_eval = load_json_file(deepseek_eval_path)

            st.markdown("#### ğŸ“Š å¹³å‡æŒ‡æ ‡å¯¹æ¯”")
            metric_cols = st.columns(3)
            local_scores = extract_scores(local_eval)
            deepseek_scores = extract_scores(deepseek_eval)

            with metric_cols[0]:
                st.markdown("**æŒ‡æ ‡**")
                st.markdown("- rouge_score")
                st.markdown("- answer_correctness")
            with metric_cols[1]:
                st.markdown("**æœ¬åœ°æ¨¡å‹**")
                st.markdown(f"- {format_score(local_scores['rouge_score']) if local_scores else 'N/A'}")
                st.markdown(f"- {format_score(local_scores['answer_correctness']) if local_scores else 'N/A'}")
            with metric_cols[2]:
                st.markdown("**DeepSeek**")
                st.markdown(f"- {format_score(deepseek_scores['rouge_score']) if deepseek_scores else 'N/A'}")
                st.markdown(f"- {format_score(deepseek_scores['answer_correctness']) if deepseek_scores else 'N/A'}")

            st.markdown("---")
            st.markdown("#### ğŸ§ª å•é¢˜å¯¹æ¯”")

            local_results = load_json_file(local_result_path)
            deepseek_results = load_json_file(deepseek_result_path)

            if not local_results or not deepseek_results:
                st.warning("æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶ï¼Œè¯·ç¡®è®¤è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚")
            else:
                local_map = {item["id"]: item for item in local_results if "id" in item}
                deepseek_map = {item["id"]: item for item in deepseek_results if "id" in item}
                common_ids = sorted(set(local_map.keys()) & set(deepseek_map.keys()))

                if not common_ids:
                    st.warning("ä¸¤ä»½ç»“æœæ²¡æœ‰é‡å çš„æ ·æœ¬IDã€‚")
                else:
                    selected_id = st.selectbox("é€‰æ‹©é—®é¢˜ID", options=common_ids)
                    local_item = local_map[selected_id]
                    deepseek_item = deepseek_map[selected_id]

                    st.markdown(f"**é—®é¢˜:** {local_item.get('question', '')}")
                    st.markdown(f"**æ ‡å‡†ç­”æ¡ˆ:** {local_item.get('ground_truth', '')}")

                    answer_cols = st.columns(2)
                    with answer_cols[0]:
                        st.markdown("**æœ¬åœ°æ¨¡å‹å›ç­”**")
                        st.text_area(
                            "local_answer",
                            value=local_item.get("generated_answer", ""),
                            height=200,
                            label_visibility="collapsed"
                        )
                    with answer_cols[1]:
                        st.markdown("**DeepSeekå›ç­”**")
                        st.text_area(
                            "deepseek_answer",
                            value=deepseek_item.get("generated_answer", ""),
                            height=200,
                            label_visibility="collapsed"
                        )

                    with st.expander("æŸ¥çœ‹æ£€ç´¢ä¸Šä¸‹æ–‡"):
                        context = local_item.get("context", [])
                        if isinstance(context, list):
                            st.write("\n\n---\n\n".join(context[:3]))
                        else:
                            st.write(context)
        # ====================================================

        # ============== ã€æ–°å¢ã€‘æ¨¡å‹å¯¹æ¯”æ ‡ç­¾é¡µ ==============
        with tab3:
            st.markdown("### âš–ï¸ æœ¬åœ°æ¨¡å‹ vs DeepSeek å¯¹æ¯”")
            st.info("âš ï¸ æ³¨æ„ï¼šDeepSeek APIåŠŸèƒ½éœ€è¦é…ç½®APIå¯†é’¥ã€‚å½“å‰ç‰ˆæœ¬ä½¿ç”¨æœ¬åœ°æ¨¡å‹è¿›è¡Œæ¼”ç¤ºå¯¹æ¯”ã€‚")

            # ä» GraphRAG-Benchmark åŠ è½½æµ‹è¯•é—®é¢˜
            questions_file = "GraphRAG-Benchmark-main/Datasets/Questions/medical_questions.json"

            if not Path(questions_file).exists():
                st.error(f"âŒ æµ‹è¯•é—®é¢˜æ–‡ä»¶ä¸å­˜åœ¨: {questions_file}")
            else:
                # åŠ è½½é—®é¢˜æ•°æ®
                with open(questions_file, 'r', encoding='utf-8') as f:
                    questions_data = json.load(f)

                st.success(f"âœ… åŠ è½½äº† {len(questions_data)} ä¸ªæµ‹è¯•é—®é¢˜")

                # é€‰æ‹©æµ‹è¯•æ¨¡å¼
                test_mode = st.radio(
                    "é€‰æ‹©æµ‹è¯•æ¨¡å¼",
                    ["å•é¢˜å¯¹æ¯”", "æ‰¹é‡è¯„ä¼° (å‰10é¢˜)"],
                    horizontal=True
                )

                if test_mode == "å•é¢˜å¯¹æ¯”":
                    st.markdown("---")
                    st.markdown("#### ğŸ“ å•é¢˜å¯¹æ¯”æµ‹è¯•")

                    # éšæœºé€‰æ‹©ä¸€ä¸ªé—®é¢˜æˆ–æ‰‹åŠ¨é€‰æ‹©
                    col1, col2 = st.columns([3, 1])
                    with col1:
                        question_idx = st.selectbox(
                            "é€‰æ‹©æµ‹è¯•é—®é¢˜",
                            range(min(50, len(questions_data))),  # åªæ˜¾ç¤ºå‰50ä¸ª
                            format_func=lambda i: f"#{i+1}: {questions_data[i]['question'][:60]}..."
                        )

                    with col2:
                        if st.button("ğŸ² éšæœºé€‰æ‹©", key="random_question"):
                            import random
                            question_idx = random.randint(0, min(49, len(questions_data)-1))
                            st.rerun()

                    if question_idx is not None:
                        test_qa = questions_data[question_idx]

                        # æ˜¾ç¤ºé—®é¢˜è¯¦æƒ…
                        st.markdown("---")
                        with st.expander("ğŸ“‹ é—®é¢˜è¯¦æƒ…", expanded=True):
                            st.markdown(f"**ID**: `{test_qa['id']}`")
                            st.markdown(f"**é—®é¢˜**: {test_qa['question']}")
                            st.markdown(f"**æ ‡å‡†ç­”æ¡ˆ**: {test_qa['answer']}")
                            if 'evidence' in test_qa:
                                st.markdown(f"**è¯æ®**: {test_qa['evidence'][:200]}...")

                        st.markdown("---")

                        if st.button("ğŸš€ å¼€å§‹å¯¹æ¯”æµ‹è¯•", key="start_comparison", type="primary"):
                            # æœç´¢ç›¸å…³æ–‡æ¡£
                            with st.spinner("æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£..."):
                                retrieved_ids, distances = search_similar_documents(
                                    milvus_client, test_qa['question'], embedding_model
                                )

                            if not retrieved_ids:
                                st.warning("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                            else:
                                # è·å–æ–‡æ¡£
                                retrieved_docs = []
                                for doc_id in retrieved_ids:
                                    if doc_id in id_to_doc_map:
                                        retrieved_docs.append(id_to_doc_map[doc_id])

                                if retrieved_docs:
                                    # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£
                                    with st.expander(f"ğŸ“š æ£€ç´¢åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³æ–‡æ¡£", expanded=False):
                                        for i, doc in enumerate(retrieved_docs[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                                            st.markdown(f"**æ–‡æ¡£{i+1}**: {doc['title']}")
                                            st.caption(doc['abstract'][:150] + "...")

                                    st.markdown("---")

                                    # å¹¶æ’å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„å›ç­”
                                    col1, col2 = st.columns(2)

                                    with col1:
                                        st.markdown("### ğŸ¤– æœ¬åœ°æ¨¡å‹å›ç­”")
                                        st.markdown(f"*æ¨¡å‹: {GENERATION_MODEL_NAME}*")

                                        with st.spinner("æœ¬åœ°æ¨¡å‹ç”Ÿæˆä¸­..."):
                                            local_answer = generate_answer(
                                                test_qa['question'],
                                                retrieved_docs,
                                                generation_model,
                                                tokenizer
                                            )

                                        st.markdown(
                                            f"""
                                            <div style="background-color:#f0f9ff;padding:1rem;border-radius:0.5rem;border-left:4px solid #0284c7;min-height:150px;">
                                                <p style="color:#0c4a6e;margin:0;">{local_answer}</p>
                                            </div>
                                            """,
                                            unsafe_allow_html=True
                                        )

                                    with col2:
                                        st.markdown("### ğŸŒ DeepSeekå›ç­”")
                                        st.markdown("*æ¨¡å‹: deepseek-chat*")

                                        # æ¨¡æ‹ŸDeepSeekå›ç­”ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦APIè°ƒç”¨ï¼‰
                                        st.info("ğŸ’¡ DeepSeek APIé›†æˆéœ€è¦é…ç½®å¯†é’¥ã€‚å½“å‰æ˜¾ç¤ºæ¨¡æ‹Ÿç»“æœã€‚")

                                        deepseek_answer = f"(æ¨¡æ‹Ÿ) {test_qa['answer']}"  # ä½¿ç”¨æ ‡å‡†ç­”æ¡ˆä½œä¸ºæ¨¡æ‹Ÿ

                                        st.markdown(
                                            f"""
                                            <div style="background-color:#fef3c7;padding:1rem;border-radius:0.5rem;border-left:4px solid #f59e0b;min-height:150px;">
                                                <p style="color:#92400e;margin:0;">{deepseek_answer}</p>
                                            </div>
                                            """,
                                            unsafe_allow_html=True
                                        )

                                    st.markdown("---")
                                    st.markdown("### ğŸ“Š è¯„ä¼°æŒ‡æ ‡")

                                    # ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦è¯„ä¼°
                                    def calculate_word_overlap(text1, text2):
                                        """è®¡ç®—è¯é‡å ç‡"""
                                        words1 = set(text1.lower().split())
                                        words2 = set(text2.lower().split())
                                        if not words1 or not words2:
                                            return 0.0
                                        overlap = len(words1 & words2)
                                        return overlap / max(len(words1), len(words2))

                                    # è®¡ç®—ä¸æ ‡å‡†ç­”æ¡ˆçš„é‡å åº¦
                                    local_overlap = calculate_word_overlap(local_answer, test_qa['answer'])
                                    deepseek_overlap = calculate_word_overlap(deepseek_answer, test_qa['answer'])

                                    col1, col2, col3 = st.columns(3)

                                    with col1:
                                        st.metric(
                                            "æœ¬åœ°æ¨¡å‹ç›¸ä¼¼åº¦",
                                            f"{local_overlap*100:.1f}%",
                                            delta=f"{(local_overlap-0.5)*100:+.1f}%"
                                        )

                                    with col2:
                                        st.metric(
                                            "DeepSeekç›¸ä¼¼åº¦",
                                            f"{deepseek_overlap*100:.1f}%",
                                            delta=f"{(deepseek_overlap-0.5)*100:+.1f}%"
                                        )

                                    with col3:
                                        winner = "æœ¬åœ°æ¨¡å‹" if local_overlap > deepseek_overlap else "DeepSeek"
                                        st.metric("æ›´ä¼˜æ¨¡å‹", winner)

                                    st.markdown("---")
                                    st.markdown("### ğŸ“Œ æ ‡å‡†ç­”æ¡ˆ")
                                    st.info(test_qa['answer'])

                                else:
                                    st.error("æ— æ³•æ£€ç´¢åˆ°æœ‰æ•ˆæ–‡æ¡£")

                else:  # æ‰¹é‡è¯„ä¼°æ¨¡å¼
                    st.markdown("---")
                    st.markdown("#### ğŸ“Š æ‰¹é‡è¯„ä¼° (å‰10é¢˜)")

                    st.info("ğŸ’¡ æ‰¹é‡è¯„ä¼°åŠŸèƒ½å°†æµ‹è¯•å‰10ä¸ªé—®é¢˜ï¼Œå¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„å¹³å‡è¡¨ç°ã€‚")

                    if st.button("ğŸš€ å¼€å§‹æ‰¹é‡è¯„ä¼°", key="batch_eval", type="primary"):
                        # æ‰¹é‡è¯„ä¼°
                        results = []
                        progress_bar = st.progress(0)
                        status_text = st.empty()

                        for i, test_qa in enumerate(questions_data[:10]):
                            status_text.text(f"æ­£åœ¨è¯„ä¼°é—®é¢˜ {i+1}/10: {test_qa['question'][:50]}...")

                            # æœç´¢æ–‡æ¡£
                            retrieved_ids, _ = search_similar_documents(
                                milvus_client, test_qa['question'], embedding_model
                            )

                            if retrieved_ids:
                                retrieved_docs = [id_to_doc_map[doc_id] for doc_id in retrieved_ids if doc_id in id_to_doc_map]

                                if retrieved_docs:
                                    # ç”Ÿæˆæœ¬åœ°æ¨¡å‹ç­”æ¡ˆ
                                    local_answer = generate_answer(
                                        test_qa['question'], retrieved_docs, generation_model, tokenizer
                                    )

                                    # æ¨¡æ‹ŸDeepSeekç­”æ¡ˆ
                                    deepseek_answer = test_qa['answer']  # ä½¿ç”¨æ ‡å‡†ç­”æ¡ˆæ¨¡æ‹Ÿ

                                    # è®¡ç®—ç›¸ä¼¼åº¦
                                    def calculate_word_overlap(text1, text2):
                                        words1 = set(text1.lower().split())
                                        words2 = set(text2.lower().split())
                                        if not words1 or not words2:
                                            return 0.0
                                        overlap = len(words1 & words2)
                                        return overlap / max(len(words1), len(words2))

                                    local_score = calculate_word_overlap(local_answer, test_qa['answer'])
                                    deepseek_score = calculate_word_overlap(deepseek_answer, test_qa['answer'])

                                    results.append({
                                        'question': test_qa['question'],
                                        'local_score': local_score,
                                        'deepseek_score': deepseek_score
                                    })

                            progress_bar.progress((i + 1) / 10)

                        status_text.text("âœ… è¯„ä¼°å®Œæˆï¼")

                        # æ˜¾ç¤ºç»“æœ
                        if results:
                            st.markdown("---")
                            st.markdown("### ğŸ“ˆ è¯„ä¼°ç»“æœæ±‡æ€»")

                            # è®¡ç®—å¹³å‡åˆ†
                            avg_local = sum(r['local_score'] for r in results) / len(results)
                            avg_deepseek = sum(r['deepseek_score'] for r in results) / len(results)

                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("æœ¬åœ°æ¨¡å‹å¹³å‡åˆ†", f"{avg_local*100:.1f}%")
                            with col2:
                                st.metric("DeepSeekå¹³å‡åˆ†", f"{avg_deepseek*100:.1f}%")
                            with col3:
                                winner = "æœ¬åœ°æ¨¡å‹" if avg_local > avg_deepseek else "DeepSeek"
                                st.metric("ç»¼åˆèƒœå‡º", winner)

                            st.markdown("---")
                            st.markdown("### ğŸ“‹ è¯¦ç»†ç»“æœ")

                            # æ˜¾ç¤ºç»“æœè¡¨æ ¼
                            import pandas as pd
                            df = pd.DataFrame(results)
                            df['question'] = df['question'].str[:60] + "..."
                            df['local_score'] = df['local_score'].apply(lambda x: f"{x*100:.1f}%")
                            df['deepseek_score'] = df['deepseek_score'].apply(lambda x: f"{x*100:.1f}%")
                            df.columns = ['é—®é¢˜', 'æœ¬åœ°æ¨¡å‹', 'DeepSeek']

                            st.dataframe(df, use_container_width=True)

                            # ä¿å­˜ç»“æœæç¤º
                            st.markdown("---")
                            st.info("ğŸ’¾ ç»“æœå·²ç”Ÿæˆï¼åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯å°†ç»“æœä¿å­˜åˆ° output/ ç›®å½•ä¸‹çš„ JSON æ–‡ä»¶ã€‚")
                        else:
                            st.warning("æœªèƒ½ç”Ÿæˆæœ‰æ•ˆè¯„ä¼°ç»“æœ")
        # ====================================================

    else:
        st.error("åŠ è½½æ¨¡å‹æˆ–è®¾ç½® Milvus Lite collection å¤±è´¥ã€‚è¯·æ£€æŸ¥æ—¥å¿—å’Œé…ç½®ã€‚")
else:
    st.error("åˆå§‹åŒ– Milvus Lite å®¢æˆ·ç«¯å¤±è´¥ã€‚è¯·æ£€æŸ¥æ—¥å¿—ã€‚")


# --- é¡µè„š/ä¿¡æ¯ä¾§è¾¹æ  ---
st.sidebar.markdown("---")
st.sidebar.header("âš™ï¸ ç³»ç»Ÿé…ç½®")
st.sidebar.markdown(f"**å‘é‡å­˜å‚¨:** Milvus Lite")
st.sidebar.markdown(f"**æ•°æ®è·¯å¾„:** `{MILVUS_LITE_DATA_PATH}`")
st.sidebar.markdown(f"**Collection:** `{COLLECTION_NAME}`")
st.sidebar.markdown(f"**æ•°æ®æ–‡ä»¶:** `{DATA_FILE}`")
st.sidebar.markdown(f"**åµŒå…¥æ¨¡å‹:** `{EMBEDDING_MODEL_NAME}`")
st.sidebar.markdown(f"**ç”Ÿæˆæ¨¡å‹:** `{GENERATION_MODEL_NAME}`")
st.sidebar.markdown(f"**æœ€å¤§ç´¢å¼•æ•°:** `{MAX_ARTICLES_TO_INDEX}`")
st.sidebar.markdown(f"**æ£€ç´¢ Top K:** `{TOP_K}`")

# ============== ã€æ–°å¢ã€‘å›¾ä¾‹è¯´æ˜ ==============
if enable_kg:
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ğŸ¨ å®ä½“ç±»å‹å›¾ä¾‹")
    st.sidebar.markdown("ğŸ”´ Disease - ç–¾ç—…")
    st.sidebar.markdown("ğŸ”µ Anatomy - è§£å‰–ç»“æ„")
    st.sidebar.markdown("ğŸŸ¢ Treatment - æ²»ç–—")
    st.sidebar.markdown("ğŸŸ  RiskFactor - é£é™©å› ç´ ")
# ====================================================
